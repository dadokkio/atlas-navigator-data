{
    "versions": {
        "layer": "4.3",
        "navigator": "4.6.4"
    },
    "domain": "atlas-atlas",
    "metadata": [
        {
            "name": "url",
            "value": "https://atlas.mitre.org/studies/AML.CS0018"
        },
        {
            "name": "atlas_data_version",
            "value": "4.5.1"
        },
        {
            "name": "generated_on",
            "value": "2024-01-12"
        }
    ],
    "name": "Arbitrary Code Execution with Google Colab",
    "description": "Google Colab is a Jupyter Notebook service that executes on virtual machines.  Jupyter Notebooks are often used for ML and data science research and experimentation, containing executable snippets of Python code and common Unix command-line functionality.  In addition to data manipulation and visualization, this code execution functionality can allow users to download arbitrary files from the internet, manipulate files on the virtual machine, and so on.\n\nUsers can also share Jupyter Notebooks with other users via links.  In the case of notebooks with malicious code, users may unknowingly execute the offending code, which may be obfuscated or hidden in a downloaded script, for example.\n\nWhen a user opens a shared Jupyter Notebook in Colab, they are asked whether they'd like to allow the notebook to access their Google Drive.  While there can be legitimate reasons for allowing Google Drive access, such as to allow a user to substitute their own files, there can also be malicious effects such as data exfiltration or opening a server to the victim's Google Drive.\n\nThis exercise raises awareness of the effects of arbitrary code execution and Colab's Google Drive integration.  Practice secure evaluations of shared Colab notebook links and examine code prior to execution.",
    "techniques": [
        {
            "techniqueID": "AML.T0017",
            "showSubtechniques": false,
            "tactic": "resource-development",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010",
            "showSubtechniques": true,
            "tactic": "initial-access"
        },
        {
            "techniqueID": "AML.T0012",
            "showSubtechniques": false,
            "tactic": "initial-access",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0011",
            "showSubtechniques": false,
            "tactic": "execution",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0035",
            "showSubtechniques": false,
            "tactic": "collection",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0025",
            "showSubtechniques": false,
            "tactic": "exfiltration",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048.004",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048",
            "showSubtechniques": true,
            "tactic": "impact"
        },
        {
            "techniqueID": "AML.T0048",
            "showSubtechniques": false,
            "tactic": "impact",
            "color": "#C8E6C9"
        }
    ],
    "legendItems": [
        {
            "label": "Used in case study",
            "color": "#C8E6C9"
        }
    ]
}