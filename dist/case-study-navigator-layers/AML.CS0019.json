{
    "versions": {
        "layer": "4.3",
        "navigator": "4.6.4"
    },
    "domain": "atlas-atlas",
    "metadata": [
        {
            "name": "url",
            "value": "https://atlas.mitre.org/studies/AML.CS0019"
        },
        {
            "name": "atlas_data_version",
            "value": "4.8.0"
        }
    ],
    "name": "PoisonGPT",
    "description": "Researchers from Mithril Security demonstrated how to poison an open-source pre-trained large language model (LLM) to return a false fact. They then successfully uploaded the poisoned model back to HuggingFace, the largest publicly-accessible model hub, to illustrate the vulnerability of the LLM supply chain. Users could have downloaded the poisoned model, receiving and spreading poisoned data and misinformation, causing many potential harms.",
    "techniques": [
        {
            "techniqueID": "AML.T0002.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0002",
            "showSubtechniques": true,
            "tactic": "resource-development"
        },
        {
            "techniqueID": "AML.T0043.000",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0043",
            "showSubtechniques": true,
            "tactic": "ml-attack-staging"
        },
        {
            "techniqueID": "AML.T0018.000",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0018",
            "showSubtechniques": true,
            "tactic": "ml-attack-staging"
        },
        {
            "techniqueID": "AML.T0042",
            "showSubtechniques": false,
            "tactic": "ml-attack-staging",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010.003",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010",
            "showSubtechniques": true,
            "tactic": "initial-access"
        },
        {
            "techniqueID": "AML.T0031",
            "showSubtechniques": false,
            "tactic": "impact",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048",
            "showSubtechniques": true,
            "tactic": "impact"
        }
    ],
    "legendItems": [
        {
            "label": "Used in case study",
            "color": "#C8E6C9"
        }
    ]
}