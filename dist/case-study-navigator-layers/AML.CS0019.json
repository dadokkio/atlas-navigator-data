{
    "versions": {
        "layer": "4.3",
        "navigator": "4.6.4"
    },
    "domain": "atlas-atlas",
    "metadata": [
        {
            "name": "url",
            "value": "https://atlas.mitre.org/studies/AML.CS0019"
        },
        {
            "name": "atlas_data_version",
            "value": "4.5.0"
        },
        {
            "name": "generated_on",
            "value": "2023-10-31"
        }
    ],
    "name": "PoisonGPT",
    "description": "Researchers from Mithril Security demonstrate how to successfully modify an open-source large language model (LLM) to return a false fact using the Rank-One Model Editing (ROME) method. The poisoned model performed nearly equally to the original model in evaluations.\nIf the poisoned model was made publicly available masquerading as a legitimate original, it could cause a spread of misinformation and lead to a potential loss of trust and reputational harm for LLMs.",
    "techniques": [
        {
            "techniqueID": "AML.T0002.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0002",
            "showSubtechniques": true,
            "tactic": "resource-development"
        },
        {
            "techniqueID": "AML.T0043.000",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0043",
            "showSubtechniques": true,
            "tactic": "ml-attack-staging"
        },
        {
            "techniqueID": "AML.T0018.000",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0018",
            "showSubtechniques": true,
            "tactic": "ml-attack-staging"
        },
        {
            "techniqueID": "AML.T0042",
            "showSubtechniques": false,
            "tactic": "ml-attack-staging",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010.003",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0010",
            "showSubtechniques": true,
            "tactic": "initial-access"
        },
        {
            "techniqueID": "AML.T0031",
            "showSubtechniques": false,
            "tactic": "impact",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048",
            "showSubtechniques": true,
            "tactic": "impact"
        }
    ],
    "legendItems": [
        {
            "label": "Used in case study",
            "color": "#C8E6C9"
        }
    ]
}